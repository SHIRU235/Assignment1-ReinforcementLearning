{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea9f705",
   "metadata": {},
   "source": [
    "## Problem 1 — Pick-and-Place Robot (MDP design)\n",
    "Goal: Formulate the pick-and-place task as an MDP and justify each component.\n",
    "\n",
    "1. States (S):\n",
    "The agent's state should capture everything needed to make decisions. A practical choice is a vector containing: joint angles (q), joint velocities (q̇), end-effector pose (position and orientation), gripper status (open/closed), and the relative positions of the object and the placement target.\n",
    "2. Actions (A):\n",
    "Use continuous actions for low-level control to achieve smooth motion. Two common choices: direct motor torques (τ) for full control; or target joint velocities or small position deltas.\n",
    "3. Transition dynamics (P):\n",
    "Transitions follow robot dynamics: s_{t+1} = f(s_t, a_t) + noise. Simulators give deterministic physics; real systems add small stochasticity.\n",
    "4. Reward function (R):\n",
    "The reward balances task completion, speed, smoothness, and safety. A compact and practical reward is:\n",
    "r_t = I_success * R_success - c_time - c_effort * ||a_t||^2 - c_smooth * ||a_t - a_{t-1}||^2 - I_collision * R_collision + c_progress * (previous_distance - current_distance).\n",
    "5. Discount factor (γ):\n",
    "Pick γ close to 1 (e.g., 0.95–0.99) since the task horizon is moderate and we want to value future success.\n",
    "6. Algorithm suggestions and practical tips:\n",
    "Best algorithms for continuous control: Soft Actor-Critic (SAC) or TD3 for off-policy learning, and PPO if you prefer an on-policy method. Use simulation, normalize inputs, clip actions, and consider pretraining with demonstrations.\n",
    "\n",
    "Note: I have chosen continuous joint-velocity style actions for safety and added both time and smoothness penalties because the assignment emphasized fast and smooth movements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5855070f",
   "metadata": {},
   "source": [
    "## Problem 2: 2x2 Gridworld \n",
    "Consider a 2x2 gridworld with four states (s1, s2, s3, s4). The agent can move up, down, left, or right. If an action leads into a wall, the agent stays in the same state. Rewards are state-based: R(s1)=5, R(s2)=10, R(s3)=1, and R(s4)=2. The initial policy always selects 'up'.  Then perform two iterations of value iteration.\n",
    "\n",
    "## Iteration 1\n",
    "## Step 1:  Initialize the value function V(s)=0 for all states.\n",
    "s1\ts2\ts3\ts4\n",
    "0\t0\t0\t0\n",
    "\n",
    "## Step 2:  Update the value function using Bellman optimality updates:\n",
    "V(s) ← max_a [ R(s) + γ * V(s′) ]\n",
    "Assume γ = 0.9.\n",
    "\n",
    "## Step 3:  Since initial V(s)=0, the first update gives:\n",
    "s1\ts2\ts3\ts4\n",
    "5\t10\t1\t2\n",
    "\n",
    "## Iteration 2\n",
    "Using the updated values from Iteration 1,  to perform another Bellman update. Now, each state value is influenced not just by its immediate reward but also by the discounted value of the best reachable next state.\n",
    "The updated values after Iteration 2 are approximately:\n",
    "s1\ts2\ts3\ts4\n",
    "14\t19\t10\t11\n",
    "\n",
    "## Conclusion\n",
    "In summary, Problem 1 demonstrated how reinforcement learning can be applied to real-world robotic tasks by designing an appropriate MDP with states, actions, and rewards. Problem 2 highlighted how value iteration works step by step, gradually improving the state-value function estimates until convergence. These problems illustrate the importance of reward design and iterative updates in teaching agents to learn effective policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76be9766",
   "metadata": {},
   "source": [
    "## Problem 3 — 5×5 Gridworld\n",
    "\n",
    "1. Environment Setup\n",
    "Implement a 5×5 Gridworld where the agent can move in four directions:\n",
    "•\tActions: R (right), D (down), L (left), U (up)\n",
    "•\tGoal: (4,4) with reward +10\n",
    "•\tGrey states: (2,2), (3,0), (0,4) with penalty -5\n",
    "•\tAll other states: -1 reward per step\n",
    "•\tDiscount factor: γ = 0.9\n",
    "Movement outside the grid leaves the agent in the same state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f94ccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.3.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Downloading numpy-2.3.3-cp313-cp313-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.6/12.8 MB 7.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.8 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.8/12.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.3/12.8 MB 7.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.9/12.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.2/12.8 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.0/12.8 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 6.6 MB/s  0:00:01\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398c37bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Value Iteration:\n",
      "Value Function:\n",
      " [[-1.3906558 -0.434062   0.62882    1.8098    -0.878    ]\n",
      " [-0.434062   0.62882    1.8098     3.122      4.58     ]\n",
      " [ 0.62882    1.8098    -0.878      4.58       6.2      ]\n",
      " [-2.1902     3.122      4.58       6.2        8.       ]\n",
      " [ 3.122      4.58       6.2        8.        10.       ]]\n",
      "Policy:\n",
      " [['R' 'R' 'R' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' 'D']\n",
      " ['R' 'D' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'R' 'G']]\n",
      "Iterations: 10, Time: 0.001759 seconds\n",
      "\n",
      "In-Place Value Iteration:\n",
      "Value Function:\n",
      " [[-1.3906558 -0.434062   0.62882    1.8098    -0.878    ]\n",
      " [-0.434062   0.62882    1.8098     3.122      4.58     ]\n",
      " [ 0.62882    1.8098    -0.878      4.58       6.2      ]\n",
      " [-2.1902     3.122      4.58       6.2        8.       ]\n",
      " [ 3.122      4.58       6.2        8.        10.       ]]\n",
      "Policy:\n",
      " [['R' 'R' 'R' 'D' 'D']\n",
      " ['R' 'R' 'R' 'R' 'D']\n",
      " ['R' 'D' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'R' 'D']\n",
      " ['R' 'R' 'R' 'R' 'G']]\n",
      "Iterations: 10, Time: 0.001624 seconds\n"
     ]
    }
   ],
   "source": [
    "# value_iteration_gridworld.py\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "ROWS, COLS = 5, 5\n",
    "ACTIONS = ['R','D','L','U']\n",
    "ACTION_TO_DELTA = {'R': (0,1), 'D': (1,0), 'L': (0,-1), 'U': (-1,0)}\n",
    "GAMMA = 0.9\n",
    "\n",
    "def in_bounds(r,c):\n",
    "    return 0 <= r < ROWS and 0 <= c < COLS\n",
    "\n",
    "def step(rc, action):\n",
    "    r,c = rc\n",
    "    dr,dc = ACTION_TO_DELTA[action]\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if in_bounds(nr, nc):\n",
    "        return (nr, nc)\n",
    "    else:\n",
    "        return (r, c)\n",
    "\n",
    "# Reward map per problem statement\n",
    "REWARDS = np.full((ROWS, COLS), -1.0)\n",
    "for (r,c) in [(2,2), (3,0), (0,4)]:  # grey states\n",
    "    REWARDS[r,c] = -5.0\n",
    "GOAL = (4,4)\n",
    "REWARDS[GOAL] = 10.0\n",
    "\n",
    "def value_iteration(rewards, gamma=GAMMA, theta=1e-5):\n",
    "    V = np.zeros_like(rewards)\n",
    "    iters = 0\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "        for r in range(ROWS):\n",
    "            for c in range(COLS):\n",
    "                if (r,c) == GOAL:\n",
    "                    V_new[r,c] = rewards[r,c]\n",
    "                    continue\n",
    "                q_vals = []\n",
    "                for a in ACTIONS:\n",
    "                    nr, nc = step((r,c), a)\n",
    "                    q_vals.append(rewards[r,c] + gamma * V[nr, nc])\n",
    "                V_new[r,c] = max(q_vals)\n",
    "                delta = max(delta, abs(V_new[r,c] - V[r,c]))\n",
    "        V = V_new\n",
    "        iters += 1\n",
    "        if delta < theta:\n",
    "            break\n",
    "    # extract greedy policy\n",
    "    policy = np.full((ROWS, COLS), '', dtype=object)\n",
    "    for r in range(ROWS):\n",
    "        for c in range(COLS):\n",
    "            if (r,c) == GOAL:\n",
    "                policy[r,c] = 'G'\n",
    "            else:\n",
    "                best_a = None\n",
    "                best_q = -1e9\n",
    "                for a in ACTIONS:\n",
    "                    nr, nc = step((r,c), a)\n",
    "                    q = rewards[r,c] + gamma * V[nr,nc]\n",
    "                    if q > best_q:\n",
    "                        best_q = q; best_a = a\n",
    "                policy[r,c] = best_a\n",
    "    return V, policy, iters\n",
    "\n",
    "def inplace_value_iteration(rewards, gamma=GAMMA, theta=1e-5):\n",
    "    V = np.zeros_like(rewards)\n",
    "    iters = 0\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        for r in range(ROWS):\n",
    "            for c in range(COLS):\n",
    "                if (r,c) == GOAL:\n",
    "                    old = V[r,c]; V[r,c] = rewards[r,c]; delta = max(delta, abs(V[r,c]-old)); continue\n",
    "                old = V[r,c]\n",
    "                q_vals = []\n",
    "                for a in ACTIONS:\n",
    "                    nr, nc = step((r,c), a)\n",
    "                    q_vals.append(rewards[r,c] + gamma * V[nr, nc])  # uses updated V if neighbor already updated\n",
    "                V[r,c] = max(q_vals)\n",
    "                delta = max(delta, abs(V[r,c] - old))\n",
    "        iters += 1\n",
    "        if delta < theta:\n",
    "            break\n",
    "    # extract policy same way\n",
    "    policy = np.full((ROWS, COLS), '', dtype=object)\n",
    "    for r in range(ROWS):\n",
    "        for c in range(COLS):\n",
    "            if (r,c) == GOAL:\n",
    "                policy[r,c] = 'G'\n",
    "            else:\n",
    "                best_a, best_q = None, -1e9\n",
    "                for a in ACTIONS:\n",
    "                    nr, nc = step((r,c), a)\n",
    "                    q = rewards[r,c] + gamma * V[nr,nc]\n",
    "                    if q > best_q: best_q = q; best_a = a\n",
    "                policy[r,c] = best_a\n",
    "    return V, policy, iters\n",
    "if __name__ == \"__main__\":\n",
    "    start = time.time()\n",
    "    V1, policy1, iters1 = value_iteration(REWARDS)\n",
    "    end = time.time()\n",
    "    print(\"Standard Value Iteration:\")\n",
    "    print(\"Value Function:\\n\", V1)\n",
    "    print(\"Policy:\\n\", policy1)\n",
    "    print(f\"Iterations: {iters1}, Time: {end-start:.6f} seconds\\n\")\n",
    "\n",
    "    start = time.time()\n",
    "    V2, policy2, iters2 = inplace_value_iteration(REWARDS)\n",
    "    end = time.time()\n",
    "    print(\"In-Place Value Iteration:\")\n",
    "    print(\"Value Function:\\n\", V2)\n",
    "    print(\"Policy:\\n\", policy2)\n",
    "    print(f\"Iterations: {iters2}, Time: {end-start:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a7dbd",
   "metadata": {},
   "source": [
    "3. Results\n",
    "\n",
    "3.1 Convergence\n",
    "\n",
    "| Method | Iterations | Time (seconds) |\n",
    "|--------|------------|----------------|\n",
    "| Standard Value Iteration | 10 | 0.0075 |\n",
    "| In-place Value Iteration | 10 | 0.0040 |\n",
    "\n",
    "Observation: In-place iteration ran slightly faster in this small grid, though both converged in the same number of iterations.\n",
    "\n",
    "3.2 Optimal Value Function V*\n",
    "\n",
    "| Row | Values |\n",
    "|-----|--------|\n",
    "| 0 | [-1.3907, -0.4341, 0.6288, 1.8098, -0.8780] |\n",
    "| 1 | [-0.4341, 0.6288, 1.8098, 3.1220, 4.5800] |\n",
    "| 2 | [0.6288, 1.8098, -0.8780, 4.5800, 6.2000] |\n",
    "| 3 | [-2.1902, 3.1220, 4.5800, 6.2000, 8.0000] |\n",
    "| 4 | [3.1220, 4.5800, 6.2000, 8.0000, 10.0000] |\n",
    "\n",
    "3.3 Optimal Greedy Policy π*\n",
    "\n",
    "| Row | Policy |\n",
    "|-----|--------|\n",
    "| 0 | ['R', 'R', 'R', 'D', 'D'] |\n",
    "| 1 | ['R', 'R', 'R', 'R', 'D'] |\n",
    "| 2 | ['R', 'D', 'R', 'R', 'D'] |\n",
    "| 3 | ['R', 'R', 'R', 'R', 'D'] |\n",
    "| 4 | ['R', 'R', 'R', 'R', 'G'] |\n",
    "\n",
    "4. In-place vs Standard Value Iteration\n",
    "\n",
    "| Aspect | Standard | In-place |\n",
    "|--------|----------|----------|\n",
    "| Iterations | 10 | 10 |\n",
    "| Time | 0.0075 s | 0.0040 s |\n",
    "| Memory | 2 arrays (old/new) | 1 array |\n",
    "| Convergence | Slightly slower in general | Slightly faster in practice for small grids |\n",
    "| Complexity | O(|S|·|A|) per iteration | O(|S|·|A|) per iteration |\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Both methods produce identical value functions and policies.\n",
    "- In-place value iteration is slightly faster in this example due to immediate updates.\n",
    "- Memory efficiency becomes more important for large state spaces; here the difference is negligible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5ff92",
   "metadata": {},
   "source": [
    "**Problem 4 — Off-policy Monte Carlo with Importance Sampling**\n",
    "\n",
    "**1.  Environment and Policies**\n",
    "\n",
    "•\tBehavior policy b(a|s): uniform random (each action probability = 0.25).\n",
    "\n",
    "•\tTarget policy π(a|s): deterministic greedy policy from value iteration.\n",
    "\n",
    "•\tDiscount factor: γ = 0.9\n",
    "\n",
    "•\tEpisodes run: 20,000\n",
    "Key point: Deterministic target + uniform behavior leads to high variance and sparse updates, making convergence slow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da31f28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Off-policy MC Estimated Value Function (V):\n",
      "[-5.6953 -5.217  -4.6856 -4.0951 -7.439 ]\n",
      "[-5.217  -4.6856 -4.0951 -3.439  -2.71  ]\n",
      "[-4.6856 -4.0951 -7.439  -2.71   -1.9   ]\n",
      "[-8.0951 -3.439  -2.71   -1.9    -1.    ]\n",
      "[-3.439 -2.71  -1.9   -1.     0.   ]\n",
      "\n",
      "Mean Squared Error vs V* (Value Iteration): 47.00496\n",
      "Execution Time: 46.6150 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##2.  Python Implementation \n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "ROWS, COLS = 5, 5\n",
    "ACTIONS = ['R','D','L','U']\n",
    "ACTION_TO_DELTA = {'R': (0,1), 'D': (1,0), 'L': (0,-1), 'U': (-1,0)}\n",
    "GAMMA = 0.9\n",
    "GOAL = (4,4)\n",
    "REWARDS = np.full((ROWS, COLS), -1.0)\n",
    "for (r,c) in [(2,2),(3,0),(0,4)]: REWARDS[r,c] = -5.0\n",
    "REWARDS[GOAL] = 10.0\n",
    "\n",
    "def step(rc, action):\n",
    "    r,c = rc\n",
    "    dr,dc = ACTION_TO_DELTA[action]\n",
    "    nr, nc = r + dr, c + dc\n",
    "    return (nr,nc) if 0<=nr<ROWS and 0<=nc<COLS else (r,c)\n",
    "\n",
    "# Value Iteration to extract target policy\n",
    "def value_iteration(rewards, gamma=GAMMA, theta=1e-5):\n",
    "    V = np.zeros_like(rewards)\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "        for r in range(ROWS):\n",
    "            for c in range(COLS):\n",
    "                if (r,c) == GOAL:\n",
    "                    V_new[r,c] = rewards[r,c]\n",
    "                    continue\n",
    "                q_vals = [rewards[r,c] + gamma * V[step((r,c),a)] for a in ACTIONS]\n",
    "                V_new[r,c] = max(q_vals)\n",
    "                delta = max(delta, abs(V_new[r,c]-V[r,c]))\n",
    "        V = V_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = np.full((ROWS,COLS), '', dtype=object)\n",
    "    for r in range(ROWS):\n",
    "        for c in range(COLS):\n",
    "            if (r,c)==GOAL: policy[r,c]='G'\n",
    "            else: policy[r,c]=max(ACTIONS, key=lambda a: rewards[r,c]+gamma*V[step((r,c),a)])\n",
    "    return V, policy\n",
    "\n",
    "# Off-policy MC with weighted importance sampling\n",
    "def off_policy_mc_IS(num_episodes, gamma=GAMMA):\n",
    "    V = np.zeros((ROWS,COLS))\n",
    "    C = np.zeros((ROWS,COLS))\n",
    "    _, target_policy = value_iteration(REWARDS, gamma)\n",
    "    for ep in range(num_episodes):\n",
    "        state = (0,0)\n",
    "        episode = []\n",
    "        while state != GOAL:\n",
    "            a = np.random.choice(ACTIONS)\n",
    "            next_state = step(state, a)\n",
    "            r = REWARDS[state]\n",
    "            episode.append((state, a, r))\n",
    "            state = next_state\n",
    "        G = 0\n",
    "        W = 1.0\n",
    "        for state, a, r in reversed(episode):\n",
    "            G = gamma*G + r\n",
    "            r_target = target_policy[state]\n",
    "            if a != r_target:\n",
    "                W = 0\n",
    "                break\n",
    "            C[state] += W\n",
    "            V[state] += (W/C[state]) * (G - V[state])\n",
    "            W *= 1.0\n",
    "    return V\n",
    "\n",
    "# Run MC and print results\n",
    "start_time = time.time()\n",
    "num_episodes = 20000\n",
    "V_mc = off_policy_mc_IS(num_episodes)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Off-policy MC Estimated Value Function (V):\")\n",
    "for row in V_mc:\n",
    "    print(np.round(row,4))\n",
    "\n",
    "V_vi, _ = value_iteration(REWARDS)\n",
    "mse = np.mean((V_mc - V_vi)**2)\n",
    "print(\"\\nMean Squared Error vs V* (Value Iteration):\", round(mse,5))\n",
    "print(\"Execution Time: {:.4f} seconds\".format(end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3d513",
   "metadata": {},
   "source": [
    "## 3. Results (20,000 Episodes)\n",
    "Estimated Value Function V_MC:\n",
    "Row\tValues\n",
    "0\t[-5.6953, -5.2170, -4.6856, -4.0951, -7.4390]\n",
    "1\t[-5.2170, -4.6856, -4.0951, -3.4390, -2.7100]\n",
    "2\t[-4.6856, -4.0951, -7.4390, -2.7100, -1.9000]\n",
    "3\t[-8.0951, -3.4390, -2.7100, -1.9000, -1.0000]\n",
    "4\t[-3.4390, -2.7100, -1.9000, -1.0000, 0.0000]\n",
    "\n",
    "Mean Squared Error vs V*:  47.00496\n",
    "Execution Time: 111.4905 seconds\n",
    "\n",
    "## 4. Discussion\n",
    "-Most states have low or negative values due to **zero-weight episodes**.  \n",
    "- Deterministic target + uniform behavior is highly inefficient.  \n",
    "- To improve: use ε-soft target, per-decision IS, or TD/actor-critic methods.  \n",
    "- Execution time is high because many episodes are generated but few contribute to updates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
